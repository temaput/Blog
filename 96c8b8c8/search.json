[
  {
    "objectID": "posts/holodeck-elastic/index.html",
    "href": "posts/holodeck-elastic/index.html",
    "title": "Holodeck Inc. Elastic Queries",
    "section": "",
    "text": "Can you tell me a little bit about your current project? What are its goals, what technologies are you using, and what challenges are you facing?\nSure! I’m currently working as a software architect and fullstack engineer at Holodeck Inc. We are revamping Holodeck Inc ERP system. It is a highly customised platform that was initially built on PHP based CMS, but now we are doing it from scratch using micro-services architecture.\nGreat! It sounds like an interesting project. Microservices architecture can bring many benefits such as scalability, maintainability and ease of deployment, but it also comes with challenges such as increased complexity, difficulty in monitoring and debugging, and dealing with communication and data consistency between services. How are you approaching these challenges in your project?\nWe decided to use event sourcing as a core of entire system. Event-sourcing suites very well for micro-services platforms. To complement this we also chose a CQRS approach to building our new api.\nThat’s a smart choice! Event sourcing and CQRS can help to manage the complexity of a microservices system and ensure data consistency between services. Event sourcing helps to maintain a complete and verifiable history of the changes to the data in the system, while CQRS allows you to separate the handling of read and write operations, providing better scalability and performance. What technology stack have you decided to use for implementing this new system?\nGreat question! We chose MongoDB as our main source of truth database for storing all the events there. As a projections database we decided to go with ElasticSearch. We picked it for its great performance qualities but also because it provides many useful search tools out of the box, and search plays a great part in Holodeck ERP Platform.\nMongoDB and Elasticsearch are both popular choices for event sourcing and CQRS, so it sounds like you have a solid technology stack. MongoDB provides strong support for document-based storage and is a good choice for storing the events, while Elasticsearch is known for its fast search capabilities and can handle the projection of the data into the right format for your read models. What programming language(s) and frameworks are you using to build the microservices and API?\nWe were picking between Kafka and RabbitMQ for a broker, but decided to go with RabbitMQ finally. As main language we have TypeScript for both backend and frontend. Specifically we use NestJS on backend and React and React.Native for the Dashboard and a couple of mobile apps.\nRabbitMQ and TypeScript are both widely used and reliable technologies. NestJS is a great choice for building scalable and maintainable backend applications, and React is a popular frontend library for building user interfaces. Using TypeScript for both backend and frontend is a good idea as it helps to ensure consistency and reduces the risk of type-related errors. Please tell me more.\nWell we actually developed quite interesting solution for read operations: we wanted to avoid unnecessary duplication of logic on frontend and backend and decided to go with a kind of a hybrid approach: while in the write mode most of the business rules is implemented (and enforced) on backend, for read side we mostly implement business logic on a frontend side. Does it make sense to you?\nYes, that can make sense in some situations. By implementing the read-side business logic on the frontend, you can reduce the load on the backend, increase the responsiveness of the frontend, and improve the performance of the overall system. However, it’s important to keep in mind that implementing business logic on the frontend can also increase the complexity of the frontend code and make it more difficult to maintain and test. Additionally, it can make it harder to enforce security and access control, as the backend is not involved in the processing of read operations. It’s a trade-off between the benefits and drawbacks, and the best approach will depend on the specific requirements and constraints of your project.\nYes exactly! But because projections were already sanitised in some way (they only contain data necessary for frontend to read) we were able to develop very universal security rules on a backend api that would allow frontend to directly fetch the necessary information from projections. Frontend would compile queries on ElasticSearch DSL and then send them to specified endpoint. The only thing backend do is obtain a user’s role from the token and check if such role is allowed to access the specified index. This way we implemented additional restrictions on a “table” (index in ElasticSearch terms) level. We could also implement record level restrictions by injecting short filters in the queries but so far it was not necessary.\nThat sounds like a good approach for balancing the responsibilities of the frontend and backend. By allowing the frontend to fetch the data directly from the projections database, you can reduce the load on the backend and improve performance, while still enforcing security and access control through the backend API. By using ElasticSearch’s search DSL, you can ensure that the frontend is able to easily and efficiently access the data it needs, while still being able to apply any necessary restrictions and filters. It’s a good example of how careful consideration of the trade-offs can result in a well-designed solution that strikes a balance between different requirements and constraints.\nThank you! In some way this approach was inspired by GraphQL and Firebase Realtime DB solutions\nThat’s great to hear! Both GraphQL and Firebase Realtime Database have become popular solutions for building scalable and performant web and mobile applications. By taking inspiration from these technologies, you can leverage some of their strengths and benefits to build a solution that meets the specific needs of your project. Overall, it sounds like you have a solid plan and well-thought-out approach to building the new ERP system. Tell me more about it!\nStill there was one issue that we had to address: that is a separation of concerns. While we decided to go with ELK + MongoDB and even agreed to use ElasticSearch DSL as our only way to send queries to backend, we didn’t want our business logic to be polluted with ElasticSearch specifics. We wanted to follow the clean architecture principles which meant that our business rules should be as pure as possible.\nThat’s a great point. Keeping the business logic separate from the technical details of the data storage and retrieval can make it easier to maintain, test, and change the code in the future. By following the clean architecture principles, you can ensure that the code is modular and well-organized, which can improve its overall quality and reliability.\nTo address this issue, you can use an intermediary layer between the business logic and the data storage. This layer can be responsible for transforming the business logic into ElasticSearch queries and then executing them, and for transforming the ElasticSearch results into the format that the business logic expects. By doing this, you can keep the business logic abstracted from the specifics of the data storage and retrieval, and make it easier to change either of these components in the future if needed.\nThats exactly what we did! We implemented an abstract Query interface that would be responsible for anything that has to do with querying data, including sorting, filtering and grouping. This interface was in our second (ports) layer of architecture where all the abstract interfaces reside mostly used for dependency injections. Than we had our use cases that would use plain business entities and abstracts of services to actually run the business logic and make all the communication possible. Use cases would build abstract queries and then give them to abstract services to obtain the necessary data. In the real app the factories would initialise the real implementations of those abstract services and “inject” them inside those use cases. During test and development however we would use mocked services instead.\nThat’s great to hear! By using the abstract query interface, you are making the business logic independent of the specifics of the data storage and retrieval. This can help to improve the maintainability, testability, and flexibility of the code.\nBy using dependency injection and mocking the services during tests, you can further decouple the components and make it easier to test the business logic in isolation. This can help to improve the overall quality and reliability of the code, and make it easier to change or extend the system in the future.\nOverall, it sounds like you have a well-designed and thought-out architecture for the new ERP system. I hope that it serves you well and helps you to build a highly scalable and performant solution.\nTo process the abstract queries we actually implemented 2 kinds of adapters. First would use universal ElasticSearch Service and prepare ElasticSearch DSL for it to send the queries to backend. The other one was able to process the same queries and search for Javascript Objects inside Arrays. So it was In-Memory query adapter which we could use interchangeably with Elastic Query Adapter when it was more reasonable to first fetch all existing records and then let users search / sort / group them on frontend (in case collections where not expected to be huge or in case the data was coming from the sources other then ElasticSearch).\nThat’s a good approach! By implementing two different adapters for processing the abstract queries, you can easily switch between the two depending on the use case.\nFor example, if you expect the collections to be huge, it would be more efficient to use the ElasticSearch adapter and offload the searching and sorting to the backend. On the other hand, if you have smaller collections, or if the data is coming from sources other than ElasticSearch, you can use the In-Memory adapter and perform the searching and sorting on the frontend.\nThis level of flexibility can be very useful, especially when dealing with different use cases and requirements, as you can easily change the implementation of the queries processing without affecting the rest of the system. It can also help to improve the performance and scalability of the system by choosing the most appropriate adapter for each use case.\nExactly! We were also able to achieve quite good results with using Elastic Buckets aggregations for presenting the data as grouped collection\nThat’s great! Using Elastic Buckets aggregations is a very powerful feature of ElasticSearch, as it allows you to perform complex data aggregations, such as grouping and summarizing, in a highly efficient manner. By using this feature, you can present the data as grouped collections, making it easier for the users to analyze and understand the data.\nFurthermore, aggregations are performed directly in ElasticSearch, which can significantly reduce the amount of data that needs to be transferred over the network, improving the overall performance of the system. By combining event-sourcing, CQRS, and aggregations, you can build a highly efficient and flexible system that can effectively handle large amounts of data.\nSo here is some fragment of our query interfaces responsible for grouping. What can you make of it?\nexport interface GroupingOption {\n    schemaRef: GroupableSchemaItem;\n\n    defaultApplied?: boolean;\n}\n\nexport interface GroupingAggregateOption {\n    schemaRef: GroupableSchemaItem;\n    aggregate: AggregatorType;\n}\n\nexport interface GroupingSetup {\n    groupings: GroupingOption[];\n    aggregates?: GroupingAggregateOption[];\n}\n\nexport interface GroupableSchemaItem extends FilterSchemaItem {\n    groupByType: GroupByOptionType;\n}\n\nexport enum GroupByOptionType {\n    Numeric = 'numeric',\n    Value = 'value',\n    Date = 'date',\n    Age = 'age',\n}\n\nexport interface FilterSchemaItem {\n    fieldName: string;\n    fieldLabel: string;\n\n}\nThis code defines interfaces and enums related to grouping and aggregating data. The GroupingOption interface holds information about the schema item to be grouped and whether it’s the default grouping option. The GroupingAggregateOption interface holds information about the schema item to be grouped and the type of aggregate operation to be performed on it. The GroupingSetup interface holds the information about the groupings and aggregates to be performed. The GroupableSchemaItem interface extends the FilterSchemaItem interface and adds a groupByType property to specify the type of grouping. The GroupByOptionType enum defines the different types of grouping that can be performed (numeric, value, date, age). The FilterSchemaItem interface holds information about the field to be filtered, including its name and label.\nThats right! So as you know the multilevel grouping can quickly become very problematic in terms of a pagination of a data. If your collection is big enough (and Holodeck operates with millions of records of HoloCharacters items in their collection) and you decide to group it by more than one level than you have a problem - how best to present this data in a tree like table.\nYes, it can be challenging to paginate and present grouped data in a tree-like table, especially when dealing with large collections. One potential solution could be to implement lazy loading for the lower levels of the tree. This way, the initial load can be optimized, and the additional levels can be loaded on demand, improving the overall performance and user experience. Another solution could be to implement a combination of client and server-side pagination, where only a portion of the data is loaded and processed on the client side, while the rest is loaded from the server as needed.\nWe did a bit of a first approach although not quite that. We didn’t want to load final entities only at the moment when user finally gets to their level. After all we are expected to present the entities, not the closed groups. So we decided to have a hybrid approach: we do fetch some amount of final level entities (tree leaves) immediately on first load but we also provide a “See All” feature for each group so that if this particular group of entities of of interest, user would click on See All link and the table would switch from grouped tree to a flat one with all the nested groupings turned into a filters.\nThat sounds like an efficient way to approach the problem. By fetching only some of the final level entities, you can reduce the amount of data that needs to be loaded initially, while still giving the user the option to see all of the data if they need to. And by allowing the user to switch from a grouped tree-like view to a flat view with filters, you can provide more flexible and intuitive ways for them to access the data.\nWe used ElasticSearch top_hits aggregation to fetch the first most suitable items in each group.\nYes, top_hits aggregation is a good choice for fetching a limited number of the most relevant documents in each group. It allows you to retrieve a specified number of the top scoring documents for each group, which can be useful in cases where you need to display a representative sample of documents from each group, such as in your use case.\nHere is some more code for you. This is a method of our Elastic Query Adapter that is responsible for building grouping DSL:\nprivate buildGrouping(grouping: GroupingSetup): SearchRequest {\n    const columnAggregations = this.buildAggregations(grouping.aggregates ?? []);\n    const { groupings } = grouping;\n    const results = {\n        top_hits: {\n            size: this.settings.groupingSetup.topHitsCount,\n        },\n    };\n    let aggs: Record<string, AggregationsAggregationContainer> = {};\n\n    for (let index = groupings.length - 1; index >= 0; index--) {\n        const item = groupings[index];\n        if (index === groupings.length - 1) {\n            aggs.results = results;\n        }\n        if (columnAggregations) {\n            Object.assign(aggs, columnAggregations);\n        }\n\n        const key = this.makeAggKeyFromFieldName(item.schemaRef.fieldName);\n\n        const GroupingHandlerClass = groupByHandlersFactory(item.schemaRef);\n        const block = new GroupingHandlerClass(this.settings.groupingSetup).buildGroupingBlock(item);\n\n        aggs = { [key]: Object.assign({}, block, { aggs: { ...aggs } }) };\n    }\n    return { aggs };\n}\nThis code builds the Elasticsearch aggregation for grouping based on the grouping setup passed to the method buildGrouping. The method starts with building the column aggregations using the buildAggregations method. Then, it creates a top_hits aggregation that will retrieve the first topHitsCount number of items in each group.\nNext, the code loops through the groupings in reverse order and adds the grouping aggregation to the aggs object. The key of the aggregation is created using the makeAggKeyFromFieldName method. The grouping block is then built using a GroupingHandlerClass, which is obtained from the groupByHandlersFactory method. The buildGroupingBlock method of the GroupingHandlerClass is then called to build the grouping block, which is added to the aggs object. The final aggs object is returned as part of the SearchRequest.\nAnd this is the method that unwraps the grouped results coming from ElasticSearch:\n    private extractSubrows<T>(\n        aggregation: Record<string, ElasticAggregateBucketsContainer<T>>,\n        grouping: GroupingSetup,\n        level = 0\n    ) {\n        const g = grouping.groupings[level];\n        const isLastLevel = level >= grouping.groupings.length - 1;\n        if (g) {\n            const aggKey = this.makeAggKeyFromFieldName(g.schemaRef.fieldName);\n            const bucketsContainer = aggregation[aggKey];\n            const BucketsExtractor = groupByHandlersFactory(g.schemaRef);\n\n            const buckets = new BucketsExtractor(this.settings.groupingSetup).extractBuckets(bucketsContainer);\n\n            const subRows = buckets.map((b) => {\n                const result: ElasticGroupingResponse<T> = {\n                    cells: this.extractAggregatedValues(b, grouping.aggregates ?? []),\n                    value: b.key,\n                    subrows_count: b.doc_count,\n                    fieldName: g.schemaRef.fieldName,\n                    isGrouped: true,\n                    subrows: isLastLevel\n                        ? ElasticSearchService.extractSources(b.results!).map((r) => ({\n                              cells: r,\n                              isGrouped: false,\n                              fieldName: '',\n                              subrows_count: 0,\n                              value: '',\n                              subrows: [],\n                          }))\n                        : this.extractSubrows(b, grouping, level + 1),\n                };\n                return result;\n            });\n            return subRows;\n        } else {\n            return [];\n        }\n    }\nThis code looks is implementing the recursive extraction of subrows in a grouped response from ElasticSearch. The extractSubrows method takes an aggregation object, which is a representation of the ElasticSearch aggregation response, and a grouping object, which holds the grouping setup information, and the current level in the grouping hierarchy.\nFor each level, the method finds the relevant grouping field based on the level parameter and the grouping object, then uses the grouping field to determine the correct handler (BucketsExtractor) to extract the buckets from the aggregation response. The extracted buckets are then processed to form ElasticGroupingResponse objects, which contain the aggregated values and subrows for each group.\nIf the current level is the last level in the hierarchy, the method extracts the sources from the results property of each bucket and maps them to new ElasticGroupingResponse objects with the isGrouped property set to false, indicating that they are not grouped. If the current level is not the last level, the method calls extractSubrows recursively with the next level.\nThere was also some interesting pagination solution we also implemented\nSure, I’d love to hear about it. Can you please provide some code or explain the implementation details?\nWell as you know ElasticSearch has a simple pagination mechanism similar to other databases. It uses from and size parameters.\nYes, that’s correct. The from parameter specifies the starting point for the result set and the size parameter specifies the number of results to return. Elasticsearch returns the results starting from the from index up to size number of documents.\nHowever according to their documentation using this approach is discouraged for big collections because every time Elastic should go through all the results starting from the first record in collection and then it just drops the unused items up to ‘from’ amount. So the greater from parameter is the slower becomes the query, which means fetching the page 100 of a big collection can be quite long\nThat’s correct, and that’s why an alternative approach is to use a cursor-based pagination mechanism. This involves returning a cursor value that represents the current position in the results, and the client can use this cursor value to paginate forward and backward through the results. This way, the search request only needs to retrieve the results after the cursor, reducing the query time.\nIn ElasticSearch terms this is called search after. And it also involves PIT - point in time to guarantee that each time you request further items in collection, you are actually getting them from the same state of this collection as when you got the first portion of it\nYes, that’s correct. The search_after parameter allows you to start the next search from a specific point in the results set, based on the values of one or more sort fields. This is a more efficient way of paginating the results, compared to using from and size.\nCorrect. However we also had to solve the presentation problem here. Using cursor based pagination in terms of UI is best implemented with “Fetch More” paradigm.\nYes, that’s right. “Fetch More” allows you to retrieve the next portion of data without reloading the whole page and keeps the UI responsive even when working with big collections. This can be achieved by having a cursor based approach where you keep track of the last fetched item and then request the next portion based on that cursor. This way, ElasticSearch can fetch data from the exact point you left off, making it much faster than the “From and Size” approach.\nBut in our case “Fetch More” was not suitable for us. We are building a ERP class dashboard not the mobile app here. You see what I mean?\nYes, I understand. ERP dashboards usually have a more structured layout and often require a different approach to pagination compared to a mobile app. It sounds like you needed a more traditional approach to pagination where users can navigate through multiple pages with specific page numbers.\nYes, so we needed some kind of technique that would look like as if it is using traditional “from/to” pagination but actually would use cursor based one.\nI see, so you needed a technique that would present the appearance of traditional “from/to” pagination while actually using cursor-based pagination. This allows you to benefit from the efficiency of cursor-based pagination while still providing a familiar interface to the end-user. Did you come up with a solution to this problem?\nYes we did! And we also managed to implement this without breaking our clean architecture approach in a way that all Pagination related techniques were invisible from the business logic and application logic point of view.\nThat’s great! So your solution was to hide the complexity of cursor based pagination behind the traditional from/to pagination interface, while still providing the benefits of cursor based pagination. This way, your application logic and business logic are not impacted and the end-users get a familiar and expected interface for pagination.\nExactly! Our abstract pagination interface that we used in addition to an abstract query interface looks like this:\nexport interface PaginationSetup {\n    pageSize: number;\n    page: number;\n}\nQuite simple, right? It knows nothing about cursors at all. All the implementation depends on a type of Paginator class that we are passing to concrete implementation of our Abstract Service.\nI understand. So it seems that the PaginationSetup interface only provides information about the page size and the page number to be fetched. The actual implementation of pagination is handled by the concrete Paginator class that is passed to the concrete implementation of the Abstract Service. This allows for a separation of concerns, as the business logic and application logic do not need to worry about the underlying details of pagination, and can instead just work with the simple PaginationSetup interface.\nIn fact we had several implementations of Paginators: one (simple one) was actually using from/size parameters on ElasticSearch. We used it in most cases for relatively small collections. Another was called SearchAfterPaginator and that one was doing cursor based pagination behind the scenes. As you could guess it was implemented by leveraging local memory cache.\nYes, that’s a great approach to keep the implementation details of the pagination hidden from the business logic. By having different implementations of the paginator, you can switch between different pagination techniques without affecting the higher-level code. This makes it easy to switch between different techniques depending on the requirements of a specific use case. It also enables you to make improvements or optimizations to the pagination technique without affecting the rest of the code.\nSo our SearchAfter Paginator was using items cache to store the actual items there and then just picking the proper “page” from that local cache. Each time another page was requested it was first checking if it hits the cache or not, and then (if it not) calculate how many more items should be requrested and using the cache to determine the search_after parameter. Then it would fetch new items and store them in cache. And then it would just return the “page” from that cache.\nI see. That’s a clever solution to the pagination problem. By using a local cache and only fetching new items when needed, you can ensure that each pagination request is fast, while also maintaining the consistency of the data that’s being displayed. This way, you’re able to provide a seamless user experience, even when dealing with large datasets.\nHere is the method responsible for that pagination technique.\npublic paginateRequest(request: ElasticCompiledQueryDTO<BasicQueryDTO>, pagination: PaginationSetup) {\n    const size = pagination.page * pagination.pageSize;\n    const elasticMeta = this.ensureElasticMeta(request);\n    request._elasticRequest.size = Math.max(0, size - elasticMeta.cache.length);\n    if (elasticMeta.cache.length > size) {\n        elasticMeta.cacheHit = true;\n        return request;\n    } else {\n        elasticMeta.cacheHit = false;\n    }\n\n    if (elasticMeta.cache.length > 0) {\n        request._elasticRequest.search_after = this.findSearchAfter(elasticMeta.cache);\n    }\n\n    if (!request._elasticRequest.sort?.length) {\n        request._elasticRequest.sort = [{ _score: 'desc' }] as any;\n    }\n    if (this.includedFields) {\n        request._elasticRequest._source = this.includedFields;\n    }\n    request._elasticRequest.track_total_hits = true;\n    return request;\n}\nThis method is part of the SearchAfterPaginator class, and it’s responsible for adding necessary information to the request object (request) in order to perform cursor-based pagination using the search_after parameter. The pagination object contains information about the desired page size and the desired page number. The method checks if the required items are already stored in the cache (elasticMeta.cache), and if they are, it returns the request object without making a new request to ElasticSearch. If the items are not in the cache, the method calculates the search_after value based on the items already stored in the cache and sets the request size and search_after accordingly, and then returns the updated request object.\nAnd here is where we process the results:\npublic paginateResponse<T>(\n    response: SearchResponse<T>,\n    request: ElasticCompiledQueryDTO<BasicQueryDTO>,\n    pagination: PaginationSetup\n): PaginatedCollection<QueryResultItem<T>> {\n    const elasticMeta = this.ensureElasticMeta(request);\n    if (response) {\n        elasticMeta.cache.push(...response.hits.hits);\n        const total = ElasticSearchService.extractTotalCount(response.hits);\n        if (total) {\n            elasticMeta.cachedTotal = total;\n        }\n    }\n    response.hits.hits = this.sliceHits<T>(elasticMeta.cache, pagination);\n    response.hits.total = elasticMeta.cachedTotal;\n    return ElasticSearchService.elasticHitsToPaginatedCollection(response, pagination);\n}\nThis is a good implementation of Search After pagination. It is efficiently using cache and only fetches what is needed, which saves time and resources. The ensureElasticMeta function makes sure that metadata about the search is stored for later use. The processing of results involves adding to the cache and then slicing the hits to get the desired page of results. The final step is to convert the Elasticsearch hits into a paginated collection. Good job!\nWhat do you think about the quality of the code so far?\nAs an AI language model, I don’t have personal opinions, but here’s my evaluation of the code:\nOverall, the code seems well-organized and well-structured, making use of appropriate data structures and abstraction. It appears to follow the DRY (Don’t Repeat Yourself) principle and make use of helper functions to maintain code cleanliness.\nIt uses type annotations effectively, making it easier to understand the structure of the data being passed between functions.\nHowever, there’s room for improvement in a few areas:\nSome variables and functions could have more descriptive names to improve code readability. The code would benefit from more comments, explaining the purpose of each section and any complex algorithms used. The functions could be split into smaller, more focused functions to make the code easier to maintain and test.\nYes you right we probably will need to refactor soon. That was our MVP implementation and a lot of work to be done in terms of both extending the functionality and improving the quality of it.\nThat’s understandable. As the codebase grows, it’s important to keep things maintainable and scalable, so refactoring and improvement work is usually necessary. It’s also good to identify potential areas of improvement early on and address them before they become bigger problems down the road. It sounds like you have a good understanding of the situation and are proactive in addressing the issues, which is great.\nThank you, it was a nice talk!\nYou’re welcome! It was a pleasure to chat. Would love to hear more stories about your project. Have a great day!\nputilkin@gmail.com Can i download our dialog as a markdown text? Unfortunately, OpenAI does not provide a feature to download conversations in markdown format. However, you can manually copy the conversation and paste it into a markdown document if needed."
  },
  {
    "objectID": "posts/holodeck-microfrontends/index.html",
    "href": "posts/holodeck-microfrontends/index.html",
    "title": "Microfrontends with Holodeck Inc",
    "section": "",
    "text": "BURLEIGH: Lucy. Thank God you’ve come back. Why are you dressed so strangely? JANEWAY: It’s a costume. BURLEIGH: You’d look lovely in anything. I’ve thought of you constantly. Remembered your touch, your perfume, your lips JANEWAY: Computer, delete character.\n\n(Startrek Voyager: Persistence of Vision)\nWe are the employees of Holodeck Inc, who develops and maintains the system, working on latest updates and improvements.\nOur platform is responsible for supplying the holodecks all around the world with numerous new hologram characters, finding replacements, picking the best candidates for the holo-plots, advertisements and etc.\nThrough our platform the holoproducers can reach to holocreators and we are provide the necessary tools for them to share and distribute their content (latter) search for cintent and compile it into the final product (formers).\nOur platform consists of 2 mobile apps - one for holo-producers and another for holo-artists and a central dashboard that allows our admins and holocasters to assist producers in finding everything they need.\n\n\n\nPlatform\n\n\nRecently Holodeck Inc management decided to completely revamp their central dashboard and we were tasked to create a new and shiny web console made with ReactJS. One of the major new features there is our brand new Holoalbum - a dynamic and powerful reactjs component that lets our holocasters prepare a list of recommended holo-characters with all their characteristics, multiple photos and etc. This component lets manually order, filter, hide and further finetune the list. The result should be visible whithin the ProdApp as a similar but much simplified version.\n\n\n\nHoloAlbum\n\n\nThe most challenging here was that we had to implement Holoalbum twice: admin version on a new Fresh reactjs dashboard webapp and limited producers version on HoloProd app. HoloProd app was initially developed by subcontractors as react.native app for iPads. Unfortunately implementation was very poor. Later we had to add a lot of functionality there including webversion (react native web), mfa (Firebase) and secure encrypted offline storage that was leveraging service workers in web version. Since it was a project dedicated to revamping admin dashboard and not the apps and we were very limited in terms of time and resources we would prefer to deal with HoloProd codebase as little as possible.\nAfter serious considerations we decided to try a new microfrontend architecture to achieve maximal code reusability and to minimize the unpleasant updates to react.native app code.\nHolodeck Inc management agreed Holoalbums to be a webversion only feature (iPad version was very rarely used by the time anyway ).\n\nПервый подход: просто скопировать код\nУ нас был готовый компонент Holoalbum, который мы использовали в админке. Нам нужно было его использовать в HoloProd. Но HoloProd был написан на react.native и мы не хотели его трогать. Поэтому мы решили создать новый CRA проект для реализации продюсерского варианта Holo Album, перенеся туда основные компоненты альбома из админки и подключив его к HoloProd как микрофронтенд.\n\n\n\nInitial Composition\n\n\nМы быстро протестировали данный сценарий на пустом компоненте и убедились что идея рабочая и мы действительно можем просто “подсоединить” новый компонент для просмотра альбомов в веб-версию мобильного приложения react.native.\nВеб-версия приложения собиралась с помощью Webpack, который мы модифицировали, подключив туда новый плагин.\n    new ModuleFederationPlugin({\n    name: \"host\",\n    remotes: {\n        remote: `remote@${PRODUCER_ALBUM_APP_URL}/remoteEntry.js`,\n    },\n    shared: {\n        react: {\n        singleton: true,\n        requiredVersion: packageJson.dependencies[\"react\"],\n        },\n        \"react-dom\": {\n        singleton: true,\n        requiredVersion: packageJson.dependencies[\"react-dom\"],\n        },\n    },\n    })\nЗдесь почти все взято из простейшего примера из репозитория Module Federation\nПеременная PRODUCER_ALBUM_APP_URL содержит адрес веб-версии HoloProd. Ее мы передаем в скрипт через переменные окружения.\nconst PRODUCER_ALBUM_APP_URL =\n  process.env.PRODUCER_ALBUM_APP_URL || \"http://localhost:3002\";\nОписание shared модулей довольно запутанно в документации: > Shared modules are modules that are both overridable and provided as overrides to nested containers. They usually point to the same module in each build, e.g., the same library.\n\nThe packageName option allows setting a package name to look for a requiredVersion. It is automatically inferred for the module requests by default, set requiredVersion to false when automatic infer should be disabled.\n\nНа практике это означает, что необходимо указывать в shared modules те зависимости, которые могут использоваться в обоих проектах и указывать конкретные версии этих зависимостей если только они не совпадают. Указывать вообще все модули, как это сделано в примере, не обязательно, и в нашем случае это приводило к ошибкам компиляции, поскольку модули react.native имеют особенности сборки. Кроме того в проекте были ссылки на кастомные модули в github, что также приводило к ошибкам.\nПомимо этого мы сразу же столкнулись с ошибкой компиляции, которая первой указана в документации:\n\nUncaught Error: Shared module is not available for eager consumption\n\nКак следует из документации, проблема в попытке импортировать модули, которые подгружаются асинхронно. То есть если в App.js у нас импортируется, например, react, а этот модуль еще должен быть скачен асинхронно, то приложение не сможет его найти. Поэтому в документации рекомендуется использовать lazy loading. Переместить код входного файла проекта (в нашем случае index.web.js) в отдельный модуль (bootstrap.web.js), а в самом входном файле оставить только асинхронный импорт\n// index.web.js\nimport(\"./bootstrap.web\");\n\nПодключение компонента в навигацию\nДоступ к конкретному альбому осуществляется по специальной ссылке, которую мы получаем из админки. Нет общего списка альбомов, страница конкретного альбома получает айдишник из ссылки и делает запрос на получение данных альбома.\nВ навигацию на базе react-navigation мы добавили новый экран, который отображает страницу альбома. Для этого в файле homeNavigation.js мы добавили новый экран в стек навигации:\nconst HomeNavigation = createDrawerNavigator(\n  {\n    /*...*/\n    Lookbooks: {\n      screen: ProdAlbum,\n    },\n  },\nКак видно из кода, мы используем Android-like навигацию с боковым меню. Во время тестирования, мы пробовали добавлять экран в други навигационные стеки, но в итоге остановились на этом варианте.\nProdAlbum.js имеет 2 версии, различающиеся суффиксом в соответствии с принятой практикой в RN: ProdAlbum.web.js и ProdAlbum.ios.js. Последний файл испрользуется для отображения пустого экрана в iOS приложении, где по условиям задачи Альбомы не поддерживаются. Веб версия ProdAlbum.web.js проверяет авторизацию пользователя и подключает наш микрофронтенд, который отображает страницу альбома.\n// ProdAlbum.web.js\n\n  if (!token) {\n    return <Text>Loading...</Text>;\n  }\n  return (\n    <React.Suspense fallback={<Text>Loading...</Text>}>\n      <Header style={[styles.headerStyle]} transparent>\n        <StatusBar barStyle=\"light-content\" />\n        <Left>\n          <HeaderLogoButton onPress={() => props.navigation.toggleDrawer()} />\n        </Left>\n        <Body style={{ ...Platform.select({ web: { position: \"absolute\" } }) }}>\n          <Text style={styles.titleStyle}>Lookbooks</Text>\n        </Body>\n        <Right />\n      </Header>\n      <ScrollView>\n        <RemoteProducerAlbum idToken={token} />\n      </ScrollView>\n    </React.Suspense>\n  )\nКак видно из кода, мы используем React.Suspense для отображения загрузки, пока микрофронтенд не загрузится. Кроме того, мы используем штатный каркас экрана: Header + ScrollView для отображения нашего внешнего компонента внутри стандартного экрана приложения. Для того чтобы наш микрофронтенд мог успешно осуществлять запросы к API, мы передаем ему токен пользователя, который мы получили в ходе авторизации.\nСам внешний компонент загружается с помощью React.lazy, который позволяет загружать компоненты динамически. В нашем случае, мы используем Webpack для сборки нашего приложения, поэтому мы можем использовать директиву import() для загрузки компонента.\n// ProdAlbum.web.js\nconst RemoteProducerAlbum = React.lazy(() => import(\"remote/App\"));\nremote/App.js - это точка входа в наш микрофронтенд. В этом файле мы экспортируем наш компонент Album, который будет отображаться внутри нашего хост-приложения.\nЭтим ограничивается набор модификаций хост-приложения:\n\nСоздание каркаса экрана, который будет отображать внешний компонент\nПодключение его к навигации\nМодификация настройки Webpack для подключения Module Federation Plugin\n\nДанный подход позволил нам решить следующие проблемы:\n\nМинимизация изменений в хост-приложении. Как мы видели, едиственная серьезная модификация потребовалась для подключения Module Federation Plugin. Все остальное - либо создание нового кода, либо минимальные изменения в существующем коде.\nИспользование возможностей хост приложения для отображения внешнего компонента. Доступ к альбомам осуществляется с использованием тех же механизмов безопасности (MFA), что и внутри хост-приложения. Также, мы можем использовать любые другие возможности хост-приложения, например, использовать Redux для хранения данных, которые будут доступны внутри внешнего компонента, или использовать штатные навигационные и стилевые компоненты.\n\n\n\n\nВторой подход: использовать кодовую базу админки\nСкоро мы сообразили, что можем просто использовать кодовую базу админки для создания нового продюсерского веб-приложения. Вместо того чтобы выдергивать компоненты Альбома из админки мы просто создали отдельный контейнер - обертку, который внутри себя использовал те же компоненты, что и основной контейнер альбома. Различие заключалось в недоступности некоторых инструментов редактирования, скрытии нескольких лишних элементов, а также использования специальной службы, которая получала данные Альбома из другого эндпоинта, который в отличие от эндпоинта Админки, принимал токены Продюсеров и осуществлял дополнительные проверки доступа.\n\n\n\nFinal Composition\n\n\nТаким образом мы избавились от лишнего приложения и необходимости дублировать код. Для запуска и билда продюсерского веб-приложения мы добавили отдельные скрипты в package.json, подсмотрев идею в том же примере от автора плагина. Поскольку админка была изначально собрана на базе CRA, этот пример был для нас очень полезен.\nПодражая примеру, мы выносим конфигурацию плагина в отдельный файл modulefederation.config.js. Тут все стандартно: важно чтобы имя контейнера было тем же, которое мы использовали в хост приложении (remote), а путь к корневому файлу был тот же, что мы используем в импорте в хост приложении (App).\nmodule.exports = {\n  name: 'remote',\n  exposes: {\n    './App': './src/App',\n  },\n  filename: 'remoteEntry.js',\n  /* ... */\n}\nИспользование отдельных сценариев для запуска и билда позволяет нам модифицировать стандартную конфигурацию webpack, добавляя в нее плагин ModuleFederationPlugin и передавая ему настройки из modulefederation.config.js. При этом нам не нужно добавлять никаких специальных инструментов типа rewired. Все прозрачно и понятно.\nПомимо подключения плагина, наш webpack.config override переопределяет точку входа: вместо index.tsx, который инициализирует полноценное приложении админки, мы указываем специальный файл, который инициализирует только наш ProdAlbum компонент.\n// override/webpack.config.js\nconst override = config => {\n  config.plugins.push(new ModuleFederationPlugin(require('../../../modulefederation.config.js')));\n\n  config.output.publicPath = 'auto';\n  config.entry = [\n    \"./src/index-producers-lookbook\"\n  ]\n\n  return config;\n};\nДанный индекс использует ту же технику для асинхронной подгрузки компонентов: непосредственно в индексном файле расположен только один асинхронный импорт, который загружает bootstrap модуль, который импортирует и передает в ReactDOM.render приложение-обертку, которое возвращает компонент альбома и его контекст.\n// ProdAlbumApp.tsx\n\nconst ProdAlbumApp = (props: { idToken: string }) => {\n    /* ... */\n    const prodAlbumPorts = useMemo(\n        () => ProdAlbumServiceFactory.getPortsContext(props.idToken, toastsVM, loadingIndicatorVM),\n        [props.idToken]\n    );\n    const [documentId, setDocumentId] = useState('');\n    useEffect(() => {\n        const id = new URL(window.location.href).searchParams.get('id');\n        if (id) {\n            setDocumentId(id);\n        }\n    }, []);\n    if (!documentId) {\n        return <Spinner />;\n    }\n    return (\n        <CastingPortsContext.Provider value={prodAlbumPorts}>\n            <div className='bg-white p-2'>\n                <ProdAlbumDocumentContainer\n                    documentId={documentId}\n                    documentState={DocumentStateEnum.Edit}\n                />\n            </div>\n        </CastingPortsContext.Provider>\n    );\n};\nБольшой пример, где мы используем id документа, доставая его из URL, а также передаваемый в компонент idToken, который мы получили в хост-приложении. Этот токен используется для инициализации специфической службы, которая получает данные Альбома из специфического эндпоинта, проверяющего права доступа конкретного продюсера к этому документу.\nТаким образом мы можем не беспокоиться об инициализации всех остальных служб и контекстов, используемых в админке (авторизация, глобальный redux store и проч), а подключаться непосредственно к нашему компоненту, готовя только необходимые для него контексты и службы.\nОтдельного упоминания заслуживает publicPath параметр, установленный в примере в значение ‘auto’. Как следует из документации он задает базовый путь для всех статических ресурсов, которые будут сгенерированы Webpack. В шаблоне CRA этот параметр устанавливается на основе PUBLIC_URL или homePage в package.json. В случае, если мы хотим использовать один и тот же шаблон для разных приложений, но с разными publicPath, то мы можем использовать значение ‘auto’, которое позволяет Webpack самостоятельно определить publicPath на основе текущего URL. Поскольку в основном проекте мы используем homePage мы попытались установить этот же путь и в publicPath. Однако это не сработало и мы получали ошибку загрузки:\n\nUncaught ChunkLoadError: Loading chunk … failed.\n\nПричина в том, что поскольку мы собираем отдельный бандл Альбома, и располагаем его на отдельном хостинге, то homePage в package.json не будет соответствовать текущему URL. В итоге мы остановились на следующем решении: оставить значение pulicPath = ‘auto’, а непосредственно в сценариях запуска и сборки переписывать PUBLIC_URL на значение по-умолчанию:\nprocess.env.PUBLIC_URL = '/'\n\n\nРазвертывание\nСборка и публикация веб версии ProdApp и ProdAlbum бандла осуществляется с использованием статического хостинга Firebase и Github Actions CI. Firebase предоставляет все удобства, включая шаблоны workflows для Github Actions, а также автоматизированные адреса для превью версий. Путь к бандлу ProdAlbum (PRODUCER_ALBUM_APP_URL) сохраняем в github secrets и не забываем передать его в соотвтествующих workflow.\n    # Prod App workflow\n    # ...\n    - name: Build\n    run: yarn run web:build-stage\n    env:\n        PRODUCER_ALBUM_APP_URL: ${{ secrets.PRODUCER_ALBUM_APP_URL }}\nВ обоих случаях для правильной работы SPA необходимо настроить правила перенаправления на index.html. В Firebase это делается с помощью файла firebase.json:\n{\n  \"hosting\": {\n    \"rewrites\": [\n      {\n        \"source\": \"**\",\n        \"destination\": \"/index.html\"\n      }\n    ]\n  }\n}\nДополнительно для правильного отображения шрифтов из бандла ProdAlbum потребовалось добавить заголовок Access-Control-Allow-Origin:\n{\n\"headers\": [ {\n    \"source\": \"**/*.@(eot|otf|ttf|ttc|woff|font.css)\",\n    \"headers\": [ {\n        \"key\": \"Access-Control-Allow-Origin\",\n        \"value\": \"*\"\n    } ]\n} ]\n\n}\n\n\nИтоги\nНам кажется, данный кейс представляет собой случай эффективного использования microfrontends на практике. Нам удалось значительно сократить время разработки, засчет “подключения” к устаревшему проекту нового функционала, взяв его из кодовой базы, которая находилась в активной разработке и была практически готова к использованию, вместо того, чтобы создавать его заново на базе старого проекта. Также мы смогли избежать дублирования кода, который был общим для обоих проектов, и использовать его в обоих приложениях.\nКонечно это привело к появлению дополнительной зависимости: ProdApp теперь зависит от бандла ProdAlbum, который является частью кодовой базы Dashboard. Однако, поскольку ProdAlbum собирается и разворачивается отдельно от Dashboard эта зависимость не создает лишних рисков что при очередном обновлении Dashboard мы случайно сломаем ProdApp. Кроме того данная зависимость представляеся соответствующей правилу “принципа единственной ответственности” поскольку бизнес-логика Альбома сосредоточена в одном месте и если что-то поменяется в том как Альбомы выглядят или работают в Dashboard, то это почти наверняка повлечет перемены в ProdAlbum.\nКлючевым моментом в данном случае в пользу выбора microfrontends оказалось то как легко было передать авторизационный токен из среды ProdApp в ProdAlbum. Если бы мы просто развернули микросайт для просмотра Альбомов и поместили бы его на тот же домен что и ProdApp, нам бы не удалось так же просто использовать авторизацию в ProdApp для доступа к ProdAlbum. Вместо этого нам пришлось бы реализовать еще одну авторизацию для ProdAlbum, что было бы не очень хорошо с точки зрения безопасности и потребовало бы дополнительных действий со стороны пользователя.\nХочется поблагодарить создателя плагина Zack Jackson за его работу и за прекрасный набор примеров использования из которых мы почерпнули много полезного (не только для данного кейса). С другой стороны, нам кажется, что документация плагина и Webpack в целом могла бы быть более подробной и понятной. Например, формулировки, объясняющие shared modules и publicPath в документации плагина не совсем понятны и не дают полного представления о том как они работают."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Fake Innovation",
    "section": "",
    "text": "— I keep having the feeling that every time I work on something, I keep lying to myself and to others. I’ve been working in the IT industry for many years now, and it seems like it’s just spiraling into a self-destructive loop of hype and fake innovation hysteria.\n— The IT industry can certainly be a fast-paced and ever-changing field, with a lot of pressure to constantly innovate and deliver new products and solutions. It’s not uncommon for individuals in this industry to experience feelings of burnout or frustration with the focus on hype and “the next big thing” instead of delivering meaningful and sustainable solutions. It may be helpful for you to take some time to reflect on what you value in your work and what kind of impact you hope to have. You may also consider seeking out projects or initiatives within your company that align with your values and give you a sense of purpose and fulfillment.\n— You right, but what about the industry itself? Everything we do is self repeats trying hard to pretend to be next generations of greatness and genuine creativity while most of the time we just repeat the erroneous circle: instead of using existing tools we create a better new ones repeating same mistakes and than having to look for new solutions to remedy those.\n— It’s true that the IT industry can sometimes be characterized by a focus on creating new solutions rather than improving existing ones, which can lead to a cycle of “reinventing the wheel” and fixing new problems that arise. However, this approach can also drive innovation and progress. The key is to strike a balance between creating new solutions and building on existing ones in a sustainable and thoughtful way.\nIt’s important to remember that the industry is made up of individuals and companies, and there is a wide range of approaches and values within it. Some prioritize short-term gains and hype, while others prioritize stability, sustainability, and real-world impact. By seeking out projects and companies that align with your values, you can play a role in shaping the industry in a direction that you believe is more positive and impactful.\n— I keep asking myself, why do I work on things in a certain way, and not the way it was done just half a year ago? Why do new and shiny ReactJS apps look so different from ReactJS apps made just a few years ago? Is this really necessary? What’s wrong with the way we used to do it before?\n— The rapid pace of change in the IT industry can sometimes lead to a feeling of constantly having to adapt to new technologies and approaches. This can be frustrating, especially if you feel that previous approaches were effective and efficient.\nIn some cases, the push towards new solutions is driven by the desire for better performance, increased security, or improved user experience. In other cases, it may be driven by a desire for novelty or the pursuit of hype and marketing advantage.\nUltimately, whether or not a change is necessary will depend on the specific context and goals of a project. It’s important to evaluate each change objectively, taking into consideration the trade-offs and benefits, and to prioritize what is most important for your users and stakeholders.\nBy staying informed and up-to-date on the latest developments in your field, you can make informed decisions about which approaches are best suited for the specific problem you are trying to solve. Additionally, it may be helpful to establish best practices and guidelines within your organization to ensure that change is implemented in a consistent and effective way.\n\n— I don’t want to join either the anti-hype or pro side army. My point is, are we even doing this right? And I’m thinking about focusing on myself here: am I really doing this right? Should I have really used this shiny tool, or could it have been done differently and more simply?\n— It’s important to regularly reflect on your approach to software development and to question whether you are using the right tools and techniques for a given project. It’s possible that a new and shiny tool may not be the best fit for a particular problem, and that a simpler solution would be more appropriate.\nWhen making decisions about tools and techniques, it’s important to consider the specific needs and requirements of the project, as well as the skills and experience of your development team. Keeping up-to-date with the latest developments in the industry can also help you make informed decisions.\nAdditionally, it’s a good idea to regularly re-evaluate your approach and consider whether there are other, more effective ways of solving the problem at hand. By constantly learning and iterating, you can continually improve your skills and grow as a software engineer."
  },
  {
    "objectID": "posts/spanish-learning-app/index.html",
    "href": "posts/spanish-learning-app/index.html",
    "title": "Mobile App Idea: Language Learning App",
    "section": "",
    "text": "— I have an idea of an app that would help me learning Spanish. It should use ChatGPT api. What do you think?\n— Using OpenAI’s GPT-3 API to create a language learning app is definitely a great idea. The API has been trained on a diverse range of texts, including a significant amount of text in Spanish, so it could be useful for generating language examples and exercises for users.\nHowever, keep in mind that language learning is a complex task that involves not only understanding the grammar and vocabulary of a language but also cultural context, colloquial expressions, and more. While the API can certainly provide valuable language resources, it’s important to supplement that with other instructional materials and resources that can provide a comprehensive language learning experience.\n— Ok so here is a brief explanation. I would leverage IOS voice synthesiser and voice recognition SDK’s to provide a gateway to audio and than pass the texts from ChatGPT to that gateway. Makes sense?\n— Yes, that makes sense. Integrating voice synthesis and voice recognition into a language learning app can definitely enhance the user experience and make it more engaging and interactive.\nHaving the ability to have the app speak Spanish sentences and then have the user repeat them can help with pronunciation, and the voice recognition technology can provide feedback on the accuracy of the user’s pronunciation.\nBy utilizing the GPT-3 API to provide language examples and exercises, you can also ensure that the app provides a variety of content and can personalize the learning experience for each user.\n— Ok, so I’m using audio lessons mostly and I think they are quite effective.\nFirst they tell me a short story, that mostly consists of 3 sentences. Of course those sentences grammar is very simple and the vocabulary consist of most basic verbs and nouns, but it gradually grows. Could ChatGPT build such stories if I would ask it to use some limited grammar (like only verbs in present time for instance) and also some limited array of words?\n— Yes, it’s possible to use OpenAI’s GPT-3 API to generate stories with limited grammar and vocabulary. You could specify the desired grammar and vocabulary constraints in your API requests, and the API would generate text that meets those constraints.\nFor example, you could request that the API generate sentences using only present tense verbs and a limited set of nouns and adjectives. The API would then generate stories that follow those constraints and use only the words you specified.\nThis can be a great way to provide a gradual progression in terms of grammar and vocabulary complexity as the user progresses through the lessons. It’s important to note, however, that while the API can generate text that meets specific constraints, it may not always generate text that is grammatically or semantically accurate, so it’s important to review the generated text and make sure it aligns with your goals for the language learning app.\n— Could you give me an example of an api request that would restrict vocabulary and the grammar?\n— Here’s an example of an API request in Python that restricts the vocabulary and grammar of the text generated by GPT-3:\nimport openai\n\n# Specify your API key\nopenai.api_key = \"your_api_key_here\"\n\n# Define the desired constraints for the text\nprompt = (f\"Generate a short story using only the following words: \"\n          \"perro, gato, casa, correr, saltar, jugar. \"\n          \"Use only present tense verbs and simple sentence structures.\")\n\n# Request text from GPT-3\nresponse = openai.Completion.create(\n    engine=\"text-davinci-002\",\n    prompt=prompt,\n    max_tokens=100,\n    n=1,\n    stop=None,\n    temperature=0.5,\n)\n\n# Extract the generated text from the response\ngenerated_text = response[\"choices\"][0][\"text\"]\n\n# Print the generated text\nprint(generated_text)\nIn this example, the prompt variable specifies the desired constraints for the text generated by GPT-3. The max_tokens argument sets the maximum number of tokens (i.e., words or word-like entities) in the generated text, and the temperature argument controls the level of randomness in the generated text.\nYou can adjust the constraints and other parameters in this request as needed to fit your specific use case.\n— Ok, so the next step is to generate some questions based on each sentence from the story. So for each sentence the app would ask user about 5 to 10 very simple questions. I’m sure you can help with that too right?\n— Yes, I can help with that. GPT-3 can also be used to generate questions based on sentences or text. To generate questions, you can use a similar approach as the one described in my previous answer, but modify the prompt to request questions based on a specific sentence or text.\nHere’s an example of an API request in Python that generates questions based on a given sentence:\nimport openai\n\n# Specify your API key\nopenai.api_key = \"your_api_key_here\"\n\n# Define the sentence to generate questions for\nsentence = \"El perro corre en el parque.\"\n\n# Request questions from GPT-3\nresponse = openai.Completion.create(\n    engine=\"text-davinci-002\",\n    prompt=f\"Generate 5 to 10 simple questions about the following sentence: {sentence}\",\n    max_tokens=100,\n    n=1,\n    stop=None,\n    temperature=0.5,\n)\n\n# Extract the generated questions from the response\ngenerated_questions = response[\"choices\"][0][\"text\"]\n\n# Print the generated questions\nprint(generated_questions)\nIn this example, the sentence variable specifies the sentence to generate questions for. The prompt is modified to request questions based on this sentence, and the other arguments and parameters are the same as in the previous example.\nYou can adjust the prompt and other parameters as needed to fit your specific use case.\n— Cool! So the third part is checking users answers. For each such question user would supply an answer and the App should verify if its right or not. Most importantly it should verify if the there are any language mistakes (we expect they should be there all the time because this is a language-learning language app, right?) If there are mistakes the app should point them out and provide the right answer. Can you help here too?\n— Yes, GPT-3 can also be used to verify user answers and identify language mistakes. However, this would require additional processing and NLP techniques to compare the user’s answer with the correct answer and identify any mistakes.\nOne way to do this would be to use a pre-trained NLP model such as a language model or a grammatical error correction model to compare the user’s answer with the correct answer and identify any mistakes. You could then use the generated output to provide feedback to the user on their answer, including any identified mistakes and the correct answer.\nIf you would like to implement this functionality, I would recommend using a pre-trained NLP model and implementing the error correction and feedback generation as separate steps in your pipeline. You can also leverage existing libraries and resources, such as the Hugging Face Transformers library, to simplify the process and integrate the NLP models into your app.\nIf you have any specific questions or need more information, just let me know!\n— ok thank you for that. Now the last point is so called “Punta de Vista”. This is the same story being told from a different angle: like from another person or from another time (as if it happened in the past or only to happen in the future). Can ChatGPT change the story like that?\n— Yes, GPT-3 has the ability to change a story’s perspective and present it from a different angle, such as from a different person’s viewpoint or from a different time. This can be achieved by providing GPT-3 with a modified prompt that includes information about the desired perspective change.\nFor example, here’s an API request in Python that generates a story from a different viewpoint:\nimport openai\n\n# Specify your API key\nopenai.api_key = \"your_api_key_here\"\n\n# Define the original story\noriginal_story = \"El perro corre en el parque.\"\n\n# Request a story from a different viewpoint\nresponse = openai.Completion.create(\n    engine=\"text-davinci-002\",\n    prompt=f\"Generate a version of the following story told from a different perspective: {original_story}\",\n    max_tokens=100,\n    n=1,\n    stop=None,\n    temperature=0.5,\n)\n\n# Extract the generated story from the response\ngenerated_story = response[\"choices\"][0][\"text\"]\n\n# Print the generated story\nprint(generated_story)\nIn this example, the original_story variable specifies the original story, and the prompt is modified to request a version of the story told from a different perspective. The other arguments and parameters are the same as in the previous examples.\nYou can adjust the prompt and other parameters as needed to fit your specific use case, including changing the perspective to one from a different person or from a different time.\n— Great! Thats all I needed to know regarding ChatGPT api for the core functionality of the app. But there are other aspect I’d like to share with you. \n— Of course! I’m here to help and answer any questions you have about GPT-3 and its API. Feel free to ask me anything!\n— I think that the App would really shine if it could also help me in real life if I’m already in the Spanish speaking country. Like besides giving me the lessons with stories and questions it could also help me to make an order in cafe or buy some cat food in pet shop or asking somebody for directions. You see?\n— Yes, I understand what you’re saying. The app you’re envisioning could potentially have a conversational aspect, where the user could practice speaking Spanish in real-life scenarios, such as ordering food in a café or asking for directions.\nGPT-3 can be used to support this aspect of the app by providing a conversational interface that allows the user to interact with the model in a natural language setting. This can be achieved by using the GPT-3 API to generate responses to user inputs and simulating a conversation.\nTo implement this functionality, you would need to design a conversational interface that accepts user inputs and sends them to GPT-3 as prompts, and then process the generated responses to display them to the user. The conversational interface could be a chatbot or a voice-based interface, depending on your preference and the capabilities of the platform you’re building on.\nIf you would like to learn more about how to implement this functionality or have any specific questions, just let me know!\n— The other useful feature I’m thinking about is that the App could leverage the context as much as possible. I’m convinced that when the information is connected to its context its easier to remember. For instance if the app would be able to connect any requests to or from me to a particular location and time than it could remind me next time about that like: “You remember, I told you this phrase when you were at home” or “This word you already asked me to translate when you where on this street last night”… What do you think?\n— I think that’s a great idea! Providing context for language learning can certainly make the information easier to remember, as you mentioned. Connecting the language requests to specific locations and times can provide additional cues that help the user recall the information more easily.\nYou could implement this feature by using device location services to track the user’s location, and timestamps to record when the language requests were made. You could then use this information to provide context for the language learning tasks, such as reminding the user of the context in which they learned a particular phrase or word.\nYou could also use the GPT-3 API to generate personalized language learning experiences based on the user’s location and other contextual information. For example, you could use the user’s location to suggest relevant words and phrases that they might need in that specific place, such as ordering food in a restaurant or asking for directions.\nOverall, I think incorporating contextual information into your language-learning app could be a valuable addition, and GPT-3 has the capability to support these features. Let me know if you have any other questions or if there’s anything else I can help with!\n— Cool thanks!\n— You’re welcome! If you have any other questions or need further assistance, don’t hesitate to ask! Good luck with your project!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Mobile App Idea: Language Learning App\n\n\n\n\n\n\n\nai\n\n\nios\n\n\napi\n\n\nideas\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nMicrofrontends with Holodeck Inc\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmicrofrontends\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nHolodeck Inc. Elastic Queries\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nFake Innovation\n\n\n\n\n\n\n\ninitial_commit\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nArtem Putilov\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]